<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://www.recherche-reproductible.fr/FR-RN/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.recherche-reproductible.fr/FR-RN/" rel="alternate" type="text/html" /><updated>2022-12-13T10:21:39+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/feed.xml</id><title type="html">Recherche Reproductible</title><subtitle>Un réseau national de chercheurs pour la promotion de la recherche reproductible.</subtitle><author><name>Recherche Reproductible</name></author><entry><title type="html">The Institute for Replication</title><link href="https://www.recherche-reproductible.fr/FR-RN/news/2022/12/12/replication-institute.html" rel="alternate" type="text/html" title="The Institute for Replication" /><published>2022-12-12T07:00:00+00:00</published><updated>2022-12-12T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/news/2022/12/12/replication-institute</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/news/2022/12/12/replication-institute.html"><![CDATA[<p>The <a href="https://i4replication.org/">Institute for Replication (I4R)</a> works to improve the credibility of science by systematically reproducing and replicating research findings in leading academic journals. Our team collaborates with researchers to:</p>

<ul>
  <li><strong>Reproductions and Replications</strong>: Promote and generate reproductions and replications. Establish an open access website to serve as a central repository containing the replications, responses by the original authors and documentation.</li>
  <li><strong>Replication Resources</strong>: Prepare standardized file structure, code and documentation aimed at facilitating reproducibility and replicability by the broader community.</li>
  <li><strong>Teaching Resources</strong>: Develop and provide access to educational material on replication and open science.</li>
  <li><strong>Dissemination</strong>: Help researchers disseminate and publish reproductions and replications.</li>
</ul>]]></content><author><name>Recherche Reproductible</name></author><category term="news" /><summary type="html"><![CDATA[The Institute for Replication (I4R) works to improve the credibility of science by systematically reproducing and replicating research findings in leading academic journals. Our team collaborates with researchers to:]]></summary></entry><entry xml:lang="english"><title type="html">Toward practical transparent verifiable and long-term reproducible research using Guix</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/12/12/GUIX.html" rel="alternate" type="text/html" title="Toward practical transparent verifiable and long-term reproducible research using Guix" /><published>2022-12-12T07:00:00+00:00</published><updated>2022-12-12T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/12/12/GUIX</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/12/12/GUIX.html"><![CDATA[<p>Reproducibility crisis urge scientists to promote transparency which allows peers to draw same conclusions after performing identical steps from hypothesis to results. Growing resources are developed to open the access to methods, data and source codes. Still, the computational environment, an interface between data and source code running analyses, is not addressed. Environments are usually described with software and library names associated with version labels or provided as an opaque container image. This is not enough to describe the complexity of the dependencies on which they rely to operate on. We describe this issue and illustrate how open tools like Guix can be used by any scientist to share their environment and allow peers to reproduce it. Some steps of research might not be fully reproducible, but at least, transparency for computation is technically addressable. These tools should be considered by scientists willing to promote transparency and open science.</p>

<p><a href="https://www.nature.com/articles/s41597-022-01720-9"><span id="Vallet2022"><span style="font-variant: small-caps">Vallet, N., Michonneau, D., and Tournier, S.</span> 2022. Toward practical transparent verifiable and long-term reproducible research using Guix. <i>Scientific Data</i> <i>9</i>, 1.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="computational" /><category term="science" /><summary type="html"><![CDATA[Reproducibility crisis urge scientists to promote transparency which allows peers to draw same conclusions after performing identical steps from hypothesis to results. Growing resources are developed to open the access to methods, data and source codes. Still, the computational environment, an interface between data and source code running analyses, is not addressed. Environments are usually described with software and library names associated with version labels or provided as an opaque container image. This is not enough to describe the complexity of the dependencies on which they rely to operate on. We describe this issue and illustrate how open tools like Guix can be used by any scientist to share their environment and allow peers to reproduce it. Some steps of research might not be fully reproducible, but at least, transparency for computation is technically addressable. These tools should be considered by scientists willing to promote transparency and open science.]]></summary></entry><entry><title type="html">Journée reproductibilité du LabEx Primes [fr]</title><link href="https://www.recherche-reproductible.fr/FR-RN/past-event/2022/12/08/journee-reproducibilite.html" rel="alternate" type="text/html" title="Journée reproductibilité du LabEx Primes [fr]" /><published>2022-12-08T07:00:00+00:00</published><updated>2022-12-08T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/past-event/2022/12/08/journee-reproducibilite</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/past-event/2022/12/08/journee-reproducibilite.html"><![CDATA[<p>Le LabEx Primes organise une journée scientifique autour de la notion de
reproductibilité. Il s’agira d’aborder la reproductibilité d’un résultat
scientifique au sens large: reproductibilité expérimentale d’une mesure
(influence de la chaine d’acquisition, instrumentation, choix des paramètres),
reproductibilité numérique (chaine de traitement) dans le contexte de l’IA,
reproductibilité d’une simulation; ainsi que tout ce qui concerne l’analyse
statistique qui est mise œuvre, le questionnement sur les sources d’incertitude
et d’erreur. La journée est ouverte à toute personne intéressée. Plus d’information et inscription sur le site: https://reprod-primes.sciencesconf.org/</p>]]></content><author><name>Recherche Reproductible</name></author><category term="past-event" /><summary type="html"><![CDATA[Le LabEx Primes organise une journée scientifique autour de la notion de reproductibilité. Il s’agira d’aborder la reproductibilité d’un résultat scientifique au sens large: reproductibilité expérimentale d’une mesure (influence de la chaine d’acquisition, instrumentation, choix des paramètres), reproductibilité numérique (chaine de traitement) dans le contexte de l’IA, reproductibilité d’une simulation; ainsi que tout ce qui concerne l’analyse statistique qui est mise œuvre, le questionnement sur les sources d’incertitude et d’erreur. La journée est ouverte à toute personne intéressée. Plus d’information et inscription sur le site: https://reprod-primes.sciencesconf.org/]]></summary></entry><entry xml:lang="english"><title type="html">Sharing the Recipe: Reproducibility and Replicability in Research Across Disciplines</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/20/sharing-recipe.html" rel="alternate" type="text/html" title="Sharing the Recipe: Reproducibility and Replicability in Research Across Disciplines" /><published>2022-11-20T07:00:00+00:00</published><updated>2022-11-20T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/20/sharing-recipe</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/20/sharing-recipe.html"><![CDATA[<p>The open and transparent documentation of scientific processes has been established as a core antecedent of free knowledge. This also holds for generating robust insights in the scope of research projects. To convince academic peers and the public, the research process must be understandable and retraceable (reproducible), and repeatable (replicable) by others, precluding the inclusion of fluke findings into the canon of insights. In this contribution, we outline what reproducibility and replicability (R&amp;R) could mean in the scope of different disciplines and traditions of research and which significance R&amp;R has for generating insights in these fields. We draw on projects conducted in the scope of the Wikimedia “Open Science Fellows Program” (Fellowship Freies Wissen), an interdisciplinary, long-running funding scheme for projects contributing to open research practices. We identify twelve implemented projects from different disciplines which primarily focused on R&amp;R, and multiple additional projects also touching on R&amp;R. From these projects, we identify patterns and synthesize them into a roadmap of how research projects can achieve R&amp;R across different disciplines. We further outline the ground covered by these projects and propose ways forward.</p>

<p><a href="https://riojournal.com/article/89980/list/8/"><span id="Rahal2022"><span style="font-variant: small-caps">Rahal, R.-M., Hamann, H., Brohmer, H., and Pethig, F.</span> 2022. Sharing the Recipe: Reproducibility and Replicability in Research Across Disciplines. <i>Research Ideas and Outcomes</i> <i>8</i>.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="science" /><summary type="html"><![CDATA[The open and transparent documentation of scientific processes has been established as a core antecedent of free knowledge. This also holds for generating robust insights in the scope of research projects. To convince academic peers and the public, the research process must be understandable and retraceable (reproducible), and repeatable (replicable) by others, precluding the inclusion of fluke findings into the canon of insights. In this contribution, we outline what reproducibility and replicability (R&amp;R) could mean in the scope of different disciplines and traditions of research and which significance R&amp;R has for generating insights in these fields. We draw on projects conducted in the scope of the Wikimedia “Open Science Fellows Program” (Fellowship Freies Wissen), an interdisciplinary, long-running funding scheme for projects contributing to open research practices. We identify twelve implemented projects from different disciplines which primarily focused on R&amp;R, and multiple additional projects also touching on R&amp;R. From these projects, we identify patterns and synthesize them into a roadmap of how research projects can achieve R&amp;R across different disciplines. We further outline the ground covered by these projects and propose ways forward.]]></summary></entry><entry xml:lang="english"><title type="html">Reproducibility efforts as a teaching tool: A pilot study</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/10/pilot-study.html" rel="alternate" type="text/html" title="Reproducibility efforts as a teaching tool: A pilot study" /><published>2022-11-10T07:00:00+00:00</published><updated>2022-11-10T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/10/pilot-study</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/10/pilot-study.html"><![CDATA[<p>The “replication crisis” is a methodological problem in which many scientific research findings have been difficult or impossible to replicate. Because the reproducibility of empirical results is an essential aspect of the scientific method, such failures endanger the credibility of theories based on them and possibly significant portions of scientific knowledge. An instance of the replication crisis, analytic replication, pertains to reproducing published results through computational reanalysis of the authors’ original data. However, direct replications are costly, time-consuming, and unrewarded in today’s publishing standards. We propose that bioinformatics and computational biology students replicate recent discoveries as part of their curriculum. Considering the above, we performed a pilot study in one of the graduate-level courses we developed and taught at our University. The course is entitled Intro to R Programming and is meant for students in our Master’s and PhD programs who have little to no programming skills. As the course emphasized real-world data analysis, we thought it would be an appropriate setting to carry out this study. The primary objective was to expose the students to real biological data analysis problems. These include locating and downloading the needed datasets, understanding any underlying conventions and annotations, understanding the analytical methods, and regenerating multiple graphs from their assigned article. The secondary goal was to determine whether the assigned articles contained sufficient information for a graduate-level student to replicate its figures. Overall, the students successfully reproduced 39% of the figures. The main obstacles were the need for more advanced programming skills and the incomplete documentation of the applied methods. Students were engaged, enthusiastic, and focused throughout the semester. We believe that this teaching approach will allow students to make fundamental scientific contributions under appropriate supervision. It will teach them about the scientific process, the importance of reporting standards, and the importance of openness.</p>

<p><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010615"><span id="Karathanasis2022"><span style="font-variant: small-caps">Karathanasis, N., Hwang, D., Heng, V., et al.</span> 2022. Reproducibility efforts as a teaching tool: A pilot study. <i>PLOS Computational Biology</i> <i>18</i>, 11, e1010615.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="neuroscience" /><summary type="html"><![CDATA[The “replication crisis” is a methodological problem in which many scientific research findings have been difficult or impossible to replicate. Because the reproducibility of empirical results is an essential aspect of the scientific method, such failures endanger the credibility of theories based on them and possibly significant portions of scientific knowledge. An instance of the replication crisis, analytic replication, pertains to reproducing published results through computational reanalysis of the authors’ original data. However, direct replications are costly, time-consuming, and unrewarded in today’s publishing standards. We propose that bioinformatics and computational biology students replicate recent discoveries as part of their curriculum. Considering the above, we performed a pilot study in one of the graduate-level courses we developed and taught at our University. The course is entitled Intro to R Programming and is meant for students in our Master’s and PhD programs who have little to no programming skills. As the course emphasized real-world data analysis, we thought it would be an appropriate setting to carry out this study. The primary objective was to expose the students to real biological data analysis problems. These include locating and downloading the needed datasets, understanding any underlying conventions and annotations, understanding the analytical methods, and regenerating multiple graphs from their assigned article. The secondary goal was to determine whether the assigned articles contained sufficient information for a graduate-level student to replicate its figures. Overall, the students successfully reproduced 39% of the figures. The main obstacles were the need for more advanced programming skills and the incomplete documentation of the applied methods. Students were engaged, enthusiastic, and focused throughout the semester. We believe that this teaching approach will allow students to make fundamental scientific contributions under appropriate supervision. It will teach them about the scientific process, the importance of reporting standards, and the importance of openness.]]></summary></entry><entry xml:lang="english"><title type="html">Highlight Results, Don’t Hide Them: Enhance interpretation, reduce biases and improve reproducibility</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/29/dont-hide-them.html" rel="alternate" type="text/html" title="Highlight Results, Don’t Hide Them: Enhance interpretation, reduce biases and improve reproducibility" /><published>2022-10-29T07:00:00+00:00</published><updated>2022-10-29T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/29/dont-hide-them</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/29/dont-hide-them.html"><![CDATA[<p>Most neuroimaging studies display results that represent only a tiny fraction of the collected data. While it is conventional to present “only the significant results” to the reader, here we suggest that this practice has several negative consequences for both reproducibility and understanding. This practice hides away most of the results of the dataset and leads to problems of selection bias and irreproducibility, both of which have been recognized as major issues in neuroimaging studies recently. Opaque, all-or-nothing thresholding, even if well-intentioned, places undue influence on arbitrary filter values, hinders clear communication of scientific results, wastes data, is antithetical to good scientific practice, and leads to conceptual inconsistencies. It is also inconsistent with the properties of the acquired data and the underlying biology being studied. Instead of presenting only a few statistically significant locations and hiding away the remaining results, we propose that studies should “highlight” the former while also showing as much as possible of the rest. This is distinct from but complementary to utilizing data sharing repositories: the initial presentation of results has an enormous impact on the interpretation of a study. We present practical examples for voxelwise, regionwise and cross-study analyses using publicly available data that was analyzed previously by 70 teams (NARPS; Botvinik-Nezer, et al., 2020), showing that it is possible to balance the goals of displaying a full set of results with providing the reader reasonably concise and “digestible” findings. In particular, the highlighting approach sheds useful light on the kind of variability present among the NARPS teams’ results, which is primarily a varied strength of agreement rather than disagreement. Using a meta-analysis built on the informative “highlighting” approach shows this relative agreement, while one using the standard “hiding” approach does not. We describe how this simple but powerful change in practice—focusing on highlighting results, rather than hiding all but the strongest ones—can help address many large concerns within the field, or at least to provide more complete information about them. We include a list of practical suggestions for results reporting to improve reproducibility, cross-study comparisons and meta-analyses.</p>

<p><a href="https://www.biorxiv.org/content/10.1101/2022.10.26.513929v2"><span id="Taylor2022"><span style="font-variant: small-caps">Taylor, P.A., Reynolds, R.C., Calhoun, V., et al.</span> 2022. Highlight Results, Don’t Hide Them: Enhance interpretation, reduce biases and improve reproducibility. <i>bioRxiv</i>.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="neuroimaging" /><summary type="html"><![CDATA[Most neuroimaging studies display results that represent only a tiny fraction of the collected data. While it is conventional to present “only the significant results” to the reader, here we suggest that this practice has several negative consequences for both reproducibility and understanding. This practice hides away most of the results of the dataset and leads to problems of selection bias and irreproducibility, both of which have been recognized as major issues in neuroimaging studies recently. Opaque, all-or-nothing thresholding, even if well-intentioned, places undue influence on arbitrary filter values, hinders clear communication of scientific results, wastes data, is antithetical to good scientific practice, and leads to conceptual inconsistencies. It is also inconsistent with the properties of the acquired data and the underlying biology being studied. Instead of presenting only a few statistically significant locations and hiding away the remaining results, we propose that studies should “highlight” the former while also showing as much as possible of the rest. This is distinct from but complementary to utilizing data sharing repositories: the initial presentation of results has an enormous impact on the interpretation of a study. We present practical examples for voxelwise, regionwise and cross-study analyses using publicly available data that was analyzed previously by 70 teams (NARPS; Botvinik-Nezer, et al., 2020), showing that it is possible to balance the goals of displaying a full set of results with providing the reader reasonably concise and “digestible” findings. In particular, the highlighting approach sheds useful light on the kind of variability present among the NARPS teams’ results, which is primarily a varied strength of agreement rather than disagreement. Using a meta-analysis built on the informative “highlighting” approach shows this relative agreement, while one using the standard “hiding” approach does not. We describe how this simple but powerful change in practice—focusing on highlighting results, rather than hiding all but the strongest ones—can help address many large concerns within the field, or at least to provide more complete information about them. We include a list of practical suggestions for results reporting to improve reproducibility, cross-study comparisons and meta-analyses.]]></summary></entry><entry xml:lang="english"><title type="html">Replication of the natural selection of bad science</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/20/Replication-of-bad-science.html" rel="alternate" type="text/html" title="Replication of the natural selection of bad science" /><published>2022-10-20T07:00:00+00:00</published><updated>2022-10-20T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/20/Replication-of-bad-science</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/20/Replication-of-bad-science.html"><![CDATA[<p>This study reports an independent replication of the findings presented by Smaldino and McElreath (2016), “The natural selection of bad science”. The replication was successful with one exception. We find that selection acting on scientist’s propensity for replication frequency caused a brief period of exuberant replication not observed in the original paper due to a coding error. This difference does not, however, change the authors’ original conclusions. We call for more replication studies for simulations as unique contributions to scientific quality assurance.</p>

<p><a href="https://osf.io/preprints/metaarxiv/sjyp3/"><span id="Kohrt2022"><span style="font-variant: small-caps">Kohrt, F., Smaldino, P.E., McElreath, R., and Schönbrodt, F.D.</span> 2022. Replication of the natural selection of bad science. .</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="anthropology" /><summary type="html"><![CDATA[This study reports an independent replication of the findings presented by Smaldino and McElreath (2016), “The natural selection of bad science”. The replication was successful with one exception. We find that selection acting on scientist’s propensity for replication frequency caused a brief period of exuberant replication not observed in the original paper due to a coding error. This difference does not, however, change the authors’ original conclusions. We call for more replication studies for simulations as unique contributions to scientific quality assurance.]]></summary></entry><entry><title type="html">Science Europe Open Science Conference 2022 [en]</title><link href="https://www.recherche-reproductible.fr/FR-RN/past-event/2022/10/18/Open-Science-Conference.html" rel="alternate" type="text/html" title="Science Europe Open Science Conference 2022 [en]" /><published>2022-10-18T07:00:00+00:00</published><updated>2022-10-18T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/past-event/2022/10/18/Open-Science-Conference</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/past-event/2022/10/18/Open-Science-Conference.html"><![CDATA[<p>Science Europe is organising its <a href="https://www.scienceeurope.org/events/open-science-conference-2022/">first conference on Open Science</a> at an important time: the COVID-19 pandemic has highlighted the value of open and collaborative research, and the November 2021 UNESCO Recommendation on Open Science has invited world regions to discuss shared values, principles, and standards.</p>

<p>At this 18 and 19 October Open Science conference, we will provide a comprehensive overview of the current policy initiatives, research assessment reforms, and financial measures that support the transition to Open Science, and look forward at new trends.</p>

<p>We will also explore the impact of the transition on the daily reality of researchers, their teams, and institutions, and discuss ways to make the transition to Open Science fair and equitable.</p>]]></content><author><name>Recherche Reproductible</name></author><category term="past-event" /><summary type="html"><![CDATA[Science Europe is organising its first conference on Open Science at an important time: the COVID-19 pandemic has highlighted the value of open and collaborative research, and the November 2021 UNESCO Recommendation on Open Science has invited world regions to discuss shared values, principles, and standards.]]></summary></entry><entry xml:lang="french"><title type="html">Intégrer les Réplications dans les Programmes de Licence : Bénéfices pour l’Enseignement, la Science, et la Société</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/06/Teaching-Replication.html" rel="alternate" type="text/html" title="Intégrer les Réplications dans les Programmes de Licence : Bénéfices pour l’Enseignement, la Science, et la Société" /><published>2022-10-06T07:00:00+00:00</published><updated>2022-10-06T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/06/Teaching-Replication</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/06/Teaching-Replication.html"><![CDATA[<p>Depuis plus d’une décennie, la recherche en psychologie a du mal à reproduire de nombreuses études bien connues et très citées. Cette crise de la réplication a été à l’origine de l’émergence de plusieurs projets scientifiques “Big Team”. L’un des premiers contributeurs au mouvement Big-Team Science est le Collaborative Replication and Education Project (CREP), un projet de collaboration à grande échelle par lequel des étudiants de premier cycle universitaire peuvent contribuer aux connaissances scientifiques par le biais d’études de réplication. Non seulement le CREP permet aux étudiants d’apprendre et de s’intéresser aux méthodes de recherche indépendamment de la nouveauté de leur étude, mais il offre également une contribution incroyable à la communauté des chercheurs dans son ensemble.</p>

<p><a href="https://psyarxiv.com/8dhbg/"><span id="Ribotta2022"><span style="font-variant: small-caps">Ribotta, B., Bellemin, R., Grahe, J.E., IJzerman, H., Wagge, J.R., and Mailliez, M.</span> 2022. Intégrer les Réplications dans les Programmes de
                  Licence : Bénéfices pour l’Enseignement, la
                  Science, et la Société. .</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="psychology" /><summary type="html"><![CDATA[Depuis plus d’une décennie, la recherche en psychologie a du mal à reproduire de nombreuses études bien connues et très citées. Cette crise de la réplication a été à l’origine de l’émergence de plusieurs projets scientifiques “Big Team”. L’un des premiers contributeurs au mouvement Big-Team Science est le Collaborative Replication and Education Project (CREP), un projet de collaboration à grande échelle par lequel des étudiants de premier cycle universitaire peuvent contribuer aux connaissances scientifiques par le biais d’études de réplication. Non seulement le CREP permet aux étudiants d’apprendre et de s’intéresser aux méthodes de recherche indépendamment de la nouveauté de leur étude, mais il offre également une contribution incroyable à la communauté des chercheurs dans son ensemble.]]></summary></entry><entry xml:lang="english"><title type="html">It’s time! Ten reasons to start replicating simulation studies</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/09/14/Ten-reasons.html" rel="alternate" type="text/html" title="It’s time! Ten reasons to start replicating simulation studies" /><published>2022-09-14T07:00:00+00:00</published><updated>2022-09-14T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/09/14/Ten-reasons</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/09/14/Ten-reasons.html"><![CDATA[<p>The quantitative analysis of research data is a core element of empirical research. The performance of statistical methods that are used for analyzing empirical data can be evaluated and compared using computer simulations. A single simulation study can influence the analyses of thousands of empirical studies to follow. With great power comes great responsibility. Here, we argue that this responsibility includes replication of simulation studies to ensure a sound foundation for data analytical decisions. Furthermore, being designed, run, and reported by humans, simulation studies face challenges similar to other experimental empirical research and hence should not be exempt from replication attempts. We highlight that the potential replicability of simulation studies is an opportunity quantitative methodology as a field should pay more attention to.</p>

<p><a href="https://www.frontiersin.org/articles/10.3389/fepid.2022.973470/full"><span id="Lohmann2022"><span style="font-variant: small-caps">Lohmann, A., Astivia, O.L.O., Morris, T.P., and Groenwold, R.H.H.</span> 2022. It’s time! Ten reasons to start replicating simulation studies. <i>Frontiers in Epidemiology</i> <i>2</i>.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="epidemiology" /><summary type="html"><![CDATA[The quantitative analysis of research data is a core element of empirical research. The performance of statistical methods that are used for analyzing empirical data can be evaluated and compared using computer simulations. A single simulation study can influence the analyses of thousands of empirical studies to follow. With great power comes great responsibility. Here, we argue that this responsibility includes replication of simulation studies to ensure a sound foundation for data analytical decisions. Furthermore, being designed, run, and reported by humans, simulation studies face challenges similar to other experimental empirical research and hence should not be exempt from replication attempts. We highlight that the potential replicability of simulation studies is an opportunity quantitative methodology as a field should pay more attention to.]]></summary></entry></feed>