<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://www.recherche-reproductible.fr/FR-RN/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.recherche-reproductible.fr/FR-RN/" rel="alternate" type="text/html" /><updated>2023-01-09T13:44:29+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/feed.xml</id><title type="html">Recherche Reproductible</title><subtitle>Un réseau national de chercheurs pour la promotion de la recherche reproductible.</subtitle><author><name>Recherche Reproductible</name></author><entry><title type="html">Recherche Reproductible: états des lieux</title><link href="https://www.recherche-reproductible.fr/FR-RN/event/2023/03/18/RR-Days.html" rel="alternate" type="text/html" title="Recherche Reproductible: états des lieux" /><published>2023-03-18T08:00:00+00:00</published><updated>2023-03-18T08:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/event/2023/03/18/RR-Days</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/event/2023/03/18/RR-Days.html"><![CDATA[<p>Un nombre croissant de chercheurs s’intéresse à la problématique de la reproductibilité, dont la définition même peut grandement varier d’une discpline à une autre (observationnelle, expérimentale, statistique, computationnelle, …). Or, nous avons rarement l’occasion de poser un regard inter-disciplinaire sur nos approches et définitions respectives. Ce <a href="http://www.recherche-reproductible.fr/rr-days/">workshop</a> se veut donc un lieu d’échanges et d’information pour dresser un premier état des lieux de la reproductibilité en France. Le but n’est donc pas tant de faire de longs exposés, mais plutôt de favoriser les interventions courtes suivies de discussion permettant à tout un chacun de mieux comprendre les problématiques des autres en général et de la jeune génération (doctorants et post-doctorants) en particulier. Nous essayons donc de rassembler des scientifiques de toutes disciplines.</p>

<p>Plus d’informations sur: <a href="http://www.recherche-reproductible.fr/rr-days">www.recherche-reproductible.fr/rr-days</a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="event" /><summary type="html"><![CDATA[Un nombre croissant de chercheurs s’intéresse à la problématique de la reproductibilité, dont la définition même peut grandement varier d’une discpline à une autre (observationnelle, expérimentale, statistique, computationnelle, …). Or, nous avons rarement l’occasion de poser un regard inter-disciplinaire sur nos approches et définitions respectives. Ce workshop se veut donc un lieu d’échanges et d’information pour dresser un premier état des lieux de la reproductibilité en France. Le but n’est donc pas tant de faire de longs exposés, mais plutôt de favoriser les interventions courtes suivies de discussion permettant à tout un chacun de mieux comprendre les problématiques des autres en général et de la jeune génération (doctorants et post-doctorants) en particulier. Nous essayons donc de rassembler des scientifiques de toutes disciplines.]]></summary></entry><entry><title type="html">Workshop on Reproducibility in Cancer Biology [en]</title><link href="https://www.recherche-reproductible.fr/FR-RN/event/2023/01/11/Cancer-Biology.html" rel="alternate" type="text/html" title="Workshop on Reproducibility in Cancer Biology [en]" /><published>2023-01-11T08:00:00+00:00</published><updated>2023-01-11T08:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/event/2023/01/11/Cancer-Biology</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/event/2023/01/11/Cancer-Biology.html"><![CDATA[<p>The lack of reproducibility in life sciences research, and particularly in Cancer Biology, is a growing concern in recent years, often referred to as ‘Reproducibility Crisis’. The Center for Open Science published in 2021, in the journal eLife, the results of its ‘Reproducibility Project: Cancer Biology’ reporting poor reproducibility of previously published results in the field of Cancer Research.</p>

<p><a href="https://oncospheremeeting.com/workshop-on-reproducibility-in-cancer-biology/">In this
workshop</a>
experts in different fields, through a transdisciplinary approach,
will discuss the causes, the implications and potential solutions for
the Reproducibility problems in Cancer Research.</p>

<p>More information at https://oncospheremeeting.com/workshop-on-reproducibility-in-cancer-biology/</p>]]></content><author><name>Recherche Reproductible</name></author><category term="event" /><summary type="html"><![CDATA[The lack of reproducibility in life sciences research, and particularly in Cancer Biology, is a growing concern in recent years, often referred to as ‘Reproducibility Crisis’. The Center for Open Science published in 2021, in the journal eLife, the results of its ‘Reproducibility Project: Cancer Biology’ reporting poor reproducibility of previously published results in the field of Cancer Research.]]></summary></entry><entry xml:lang="english"><title type="html">Benchopt: Reproducible, efficient and collaborative optimization benchmarks</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/09/Benchopt.html" rel="alternate" type="text/html" title="Benchopt: Reproducible, efficient and collaborative optimization benchmarks" /><published>2023-01-09T07:00:00+00:00</published><updated>2023-01-09T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/09/Benchopt</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/09/Benchopt.html"><![CDATA[<p>Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks:
L2-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details.</p>

<p><a href="https://openreview.net/forum?id=1uSzacpyWLH"><span id="moreau2022benchopt"><span style="font-variant: small-caps">Moreau, T., Massias, M., Gramfort, A., et al.</span> 2022. Benchopt: Reproducible, efficient and collaborative optimization benchmarks. <i>NeuRIPS</i>.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="computational" /><category term="science" /><summary type="html"><![CDATA[Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks: L2-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details.]]></summary></entry><entry><title type="html">The Institute for Replication</title><link href="https://www.recherche-reproductible.fr/FR-RN/news/2022/12/12/replication-institute.html" rel="alternate" type="text/html" title="The Institute for Replication" /><published>2022-12-12T07:00:00+00:00</published><updated>2022-12-12T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/news/2022/12/12/replication-institute</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/news/2022/12/12/replication-institute.html"><![CDATA[<p>The <a href="https://i4replication.org/">Institute for Replication (I4R)</a> works to improve the credibility of science by systematically reproducing and replicating research findings in leading academic journals. Our team collaborates with researchers to:</p>

<ul>
  <li><strong>Reproductions and Replications</strong>: Promote and generate reproductions and replications. Establish an open access website to serve as a central repository containing the replications, responses by the original authors and documentation.</li>
  <li><strong>Replication Resources</strong>: Prepare standardized file structure, code and documentation aimed at facilitating reproducibility and replicability by the broader community.</li>
  <li><strong>Teaching Resources</strong>: Develop and provide access to educational material on replication and open science.</li>
  <li><strong>Dissemination</strong>: Help researchers disseminate and publish reproductions and replications.</li>
</ul>]]></content><author><name>Recherche Reproductible</name></author><category term="news" /><summary type="html"><![CDATA[The Institute for Replication (I4R) works to improve the credibility of science by systematically reproducing and replicating research findings in leading academic journals. Our team collaborates with researchers to:]]></summary></entry><entry xml:lang="english"><title type="html">Toward practical transparent verifiable and long-term reproducible research using Guix</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/12/12/GUIX.html" rel="alternate" type="text/html" title="Toward practical transparent verifiable and long-term reproducible research using Guix" /><published>2022-12-12T07:00:00+00:00</published><updated>2022-12-12T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/12/12/GUIX</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/12/12/GUIX.html"><![CDATA[<p>Reproducibility crisis urge scientists to promote transparency which allows peers to draw same conclusions after performing identical steps from hypothesis to results. Growing resources are developed to open the access to methods, data and source codes. Still, the computational environment, an interface between data and source code running analyses, is not addressed. Environments are usually described with software and library names associated with version labels or provided as an opaque container image. This is not enough to describe the complexity of the dependencies on which they rely to operate on. We describe this issue and illustrate how open tools like Guix can be used by any scientist to share their environment and allow peers to reproduce it. Some steps of research might not be fully reproducible, but at least, transparency for computation is technically addressable. These tools should be considered by scientists willing to promote transparency and open science.</p>

<p><a href="https://www.nature.com/articles/s41597-022-01720-9"><span id="Vallet2022"><span style="font-variant: small-caps">Vallet, N., Michonneau, D., and Tournier, S.</span> 2022. Toward practical transparent verifiable and long-term reproducible research using Guix. <i>Scientific Data</i> <i>9</i>, 1.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="computational" /><category term="science" /><summary type="html"><![CDATA[Reproducibility crisis urge scientists to promote transparency which allows peers to draw same conclusions after performing identical steps from hypothesis to results. Growing resources are developed to open the access to methods, data and source codes. Still, the computational environment, an interface between data and source code running analyses, is not addressed. Environments are usually described with software and library names associated with version labels or provided as an opaque container image. This is not enough to describe the complexity of the dependencies on which they rely to operate on. We describe this issue and illustrate how open tools like Guix can be used by any scientist to share their environment and allow peers to reproduce it. Some steps of research might not be fully reproducible, but at least, transparency for computation is technically addressable. These tools should be considered by scientists willing to promote transparency and open science.]]></summary></entry><entry><title type="html">Journée reproductibilité du LabEx Primes [fr]</title><link href="https://www.recherche-reproductible.fr/FR-RN/past-event/2022/12/08/journee-reproducibilite.html" rel="alternate" type="text/html" title="Journée reproductibilité du LabEx Primes [fr]" /><published>2022-12-08T07:00:00+00:00</published><updated>2022-12-08T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/past-event/2022/12/08/journee-reproducibilite</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/past-event/2022/12/08/journee-reproducibilite.html"><![CDATA[<p>Le LabEx Primes organise une journée scientifique autour de la notion de
reproductibilité. Il s’agira d’aborder la reproductibilité d’un résultat
scientifique au sens large: reproductibilité expérimentale d’une mesure
(influence de la chaine d’acquisition, instrumentation, choix des paramètres),
reproductibilité numérique (chaine de traitement) dans le contexte de l’IA,
reproductibilité d’une simulation; ainsi que tout ce qui concerne l’analyse
statistique qui est mise œuvre, le questionnement sur les sources d’incertitude
et d’erreur. La journée est ouverte à toute personne intéressée. Plus d’information et inscription sur le site: https://reprod-primes.sciencesconf.org/</p>]]></content><author><name>Recherche Reproductible</name></author><category term="past-event" /><summary type="html"><![CDATA[Le LabEx Primes organise une journée scientifique autour de la notion de reproductibilité. Il s’agira d’aborder la reproductibilité d’un résultat scientifique au sens large: reproductibilité expérimentale d’une mesure (influence de la chaine d’acquisition, instrumentation, choix des paramètres), reproductibilité numérique (chaine de traitement) dans le contexte de l’IA, reproductibilité d’une simulation; ainsi que tout ce qui concerne l’analyse statistique qui est mise œuvre, le questionnement sur les sources d’incertitude et d’erreur. La journée est ouverte à toute personne intéressée. Plus d’information et inscription sur le site: https://reprod-primes.sciencesconf.org/]]></summary></entry><entry xml:lang="english"><title type="html">Sharing the Recipe: Reproducibility and Replicability in Research Across Disciplines</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/20/sharing-recipe.html" rel="alternate" type="text/html" title="Sharing the Recipe: Reproducibility and Replicability in Research Across Disciplines" /><published>2022-11-20T07:00:00+00:00</published><updated>2022-11-20T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/20/sharing-recipe</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/20/sharing-recipe.html"><![CDATA[<p>The open and transparent documentation of scientific processes has been established as a core antecedent of free knowledge. This also holds for generating robust insights in the scope of research projects. To convince academic peers and the public, the research process must be understandable and retraceable (reproducible), and repeatable (replicable) by others, precluding the inclusion of fluke findings into the canon of insights. In this contribution, we outline what reproducibility and replicability (R&amp;R) could mean in the scope of different disciplines and traditions of research and which significance R&amp;R has for generating insights in these fields. We draw on projects conducted in the scope of the Wikimedia “Open Science Fellows Program” (Fellowship Freies Wissen), an interdisciplinary, long-running funding scheme for projects contributing to open research practices. We identify twelve implemented projects from different disciplines which primarily focused on R&amp;R, and multiple additional projects also touching on R&amp;R. From these projects, we identify patterns and synthesize them into a roadmap of how research projects can achieve R&amp;R across different disciplines. We further outline the ground covered by these projects and propose ways forward.</p>

<p><a href="https://riojournal.com/article/89980/list/8/"><span id="Rahal2022"><span style="font-variant: small-caps">Rahal, R.-M., Hamann, H., Brohmer, H., and Pethig, F.</span> 2022. Sharing the Recipe: Reproducibility and Replicability in Research Across Disciplines. <i>Research Ideas and Outcomes</i> <i>8</i>.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="science" /><summary type="html"><![CDATA[The open and transparent documentation of scientific processes has been established as a core antecedent of free knowledge. This also holds for generating robust insights in the scope of research projects. To convince academic peers and the public, the research process must be understandable and retraceable (reproducible), and repeatable (replicable) by others, precluding the inclusion of fluke findings into the canon of insights. In this contribution, we outline what reproducibility and replicability (R&amp;R) could mean in the scope of different disciplines and traditions of research and which significance R&amp;R has for generating insights in these fields. We draw on projects conducted in the scope of the Wikimedia “Open Science Fellows Program” (Fellowship Freies Wissen), an interdisciplinary, long-running funding scheme for projects contributing to open research practices. We identify twelve implemented projects from different disciplines which primarily focused on R&amp;R, and multiple additional projects also touching on R&amp;R. From these projects, we identify patterns and synthesize them into a roadmap of how research projects can achieve R&amp;R across different disciplines. We further outline the ground covered by these projects and propose ways forward.]]></summary></entry><entry xml:lang="english"><title type="html">Reproducibility efforts as a teaching tool: A pilot study</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/10/pilot-study.html" rel="alternate" type="text/html" title="Reproducibility efforts as a teaching tool: A pilot study" /><published>2022-11-10T07:00:00+00:00</published><updated>2022-11-10T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/10/pilot-study</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/11/10/pilot-study.html"><![CDATA[<p>The “replication crisis” is a methodological problem in which many scientific research findings have been difficult or impossible to replicate. Because the reproducibility of empirical results is an essential aspect of the scientific method, such failures endanger the credibility of theories based on them and possibly significant portions of scientific knowledge. An instance of the replication crisis, analytic replication, pertains to reproducing published results through computational reanalysis of the authors’ original data. However, direct replications are costly, time-consuming, and unrewarded in today’s publishing standards. We propose that bioinformatics and computational biology students replicate recent discoveries as part of their curriculum. Considering the above, we performed a pilot study in one of the graduate-level courses we developed and taught at our University. The course is entitled Intro to R Programming and is meant for students in our Master’s and PhD programs who have little to no programming skills. As the course emphasized real-world data analysis, we thought it would be an appropriate setting to carry out this study. The primary objective was to expose the students to real biological data analysis problems. These include locating and downloading the needed datasets, understanding any underlying conventions and annotations, understanding the analytical methods, and regenerating multiple graphs from their assigned article. The secondary goal was to determine whether the assigned articles contained sufficient information for a graduate-level student to replicate its figures. Overall, the students successfully reproduced 39% of the figures. The main obstacles were the need for more advanced programming skills and the incomplete documentation of the applied methods. Students were engaged, enthusiastic, and focused throughout the semester. We believe that this teaching approach will allow students to make fundamental scientific contributions under appropriate supervision. It will teach them about the scientific process, the importance of reporting standards, and the importance of openness.</p>

<p><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010615"><span id="Karathanasis2022"><span style="font-variant: small-caps">Karathanasis, N., Hwang, D., Heng, V., et al.</span> 2022. Reproducibility efforts as a teaching tool: A pilot study. <i>PLOS Computational Biology</i> <i>18</i>, 11, e1010615.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="neuroscience" /><summary type="html"><![CDATA[The “replication crisis” is a methodological problem in which many scientific research findings have been difficult or impossible to replicate. Because the reproducibility of empirical results is an essential aspect of the scientific method, such failures endanger the credibility of theories based on them and possibly significant portions of scientific knowledge. An instance of the replication crisis, analytic replication, pertains to reproducing published results through computational reanalysis of the authors’ original data. However, direct replications are costly, time-consuming, and unrewarded in today’s publishing standards. We propose that bioinformatics and computational biology students replicate recent discoveries as part of their curriculum. Considering the above, we performed a pilot study in one of the graduate-level courses we developed and taught at our University. The course is entitled Intro to R Programming and is meant for students in our Master’s and PhD programs who have little to no programming skills. As the course emphasized real-world data analysis, we thought it would be an appropriate setting to carry out this study. The primary objective was to expose the students to real biological data analysis problems. These include locating and downloading the needed datasets, understanding any underlying conventions and annotations, understanding the analytical methods, and regenerating multiple graphs from their assigned article. The secondary goal was to determine whether the assigned articles contained sufficient information for a graduate-level student to replicate its figures. Overall, the students successfully reproduced 39% of the figures. The main obstacles were the need for more advanced programming skills and the incomplete documentation of the applied methods. Students were engaged, enthusiastic, and focused throughout the semester. We believe that this teaching approach will allow students to make fundamental scientific contributions under appropriate supervision. It will teach them about the scientific process, the importance of reporting standards, and the importance of openness.]]></summary></entry><entry xml:lang="english"><title type="html">Highlight Results, Don’t Hide Them: Enhance interpretation, reduce biases and improve reproducibility</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/29/dont-hide-them.html" rel="alternate" type="text/html" title="Highlight Results, Don’t Hide Them: Enhance interpretation, reduce biases and improve reproducibility" /><published>2022-10-29T07:00:00+00:00</published><updated>2022-10-29T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/29/dont-hide-them</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/29/dont-hide-them.html"><![CDATA[<p>Most neuroimaging studies display results that represent only a tiny fraction of the collected data. While it is conventional to present “only the significant results” to the reader, here we suggest that this practice has several negative consequences for both reproducibility and understanding. This practice hides away most of the results of the dataset and leads to problems of selection bias and irreproducibility, both of which have been recognized as major issues in neuroimaging studies recently. Opaque, all-or-nothing thresholding, even if well-intentioned, places undue influence on arbitrary filter values, hinders clear communication of scientific results, wastes data, is antithetical to good scientific practice, and leads to conceptual inconsistencies. It is also inconsistent with the properties of the acquired data and the underlying biology being studied. Instead of presenting only a few statistically significant locations and hiding away the remaining results, we propose that studies should “highlight” the former while also showing as much as possible of the rest. This is distinct from but complementary to utilizing data sharing repositories: the initial presentation of results has an enormous impact on the interpretation of a study. We present practical examples for voxelwise, regionwise and cross-study analyses using publicly available data that was analyzed previously by 70 teams (NARPS; Botvinik-Nezer, et al., 2020), showing that it is possible to balance the goals of displaying a full set of results with providing the reader reasonably concise and “digestible” findings. In particular, the highlighting approach sheds useful light on the kind of variability present among the NARPS teams’ results, which is primarily a varied strength of agreement rather than disagreement. Using a meta-analysis built on the informative “highlighting” approach shows this relative agreement, while one using the standard “hiding” approach does not. We describe how this simple but powerful change in practice—focusing on highlighting results, rather than hiding all but the strongest ones—can help address many large concerns within the field, or at least to provide more complete information about them. We include a list of practical suggestions for results reporting to improve reproducibility, cross-study comparisons and meta-analyses.</p>

<p><a href="https://www.biorxiv.org/content/10.1101/2022.10.26.513929v2"><span id="Taylor2022"><span style="font-variant: small-caps">Taylor, P.A., Reynolds, R.C., Calhoun, V., et al.</span> 2022. Highlight Results, Don’t Hide Them: Enhance interpretation, reduce biases and improve reproducibility. <i>bioRxiv</i>.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="neuroimaging" /><summary type="html"><![CDATA[Most neuroimaging studies display results that represent only a tiny fraction of the collected data. While it is conventional to present “only the significant results” to the reader, here we suggest that this practice has several negative consequences for both reproducibility and understanding. This practice hides away most of the results of the dataset and leads to problems of selection bias and irreproducibility, both of which have been recognized as major issues in neuroimaging studies recently. Opaque, all-or-nothing thresholding, even if well-intentioned, places undue influence on arbitrary filter values, hinders clear communication of scientific results, wastes data, is antithetical to good scientific practice, and leads to conceptual inconsistencies. It is also inconsistent with the properties of the acquired data and the underlying biology being studied. Instead of presenting only a few statistically significant locations and hiding away the remaining results, we propose that studies should “highlight” the former while also showing as much as possible of the rest. This is distinct from but complementary to utilizing data sharing repositories: the initial presentation of results has an enormous impact on the interpretation of a study. We present practical examples for voxelwise, regionwise and cross-study analyses using publicly available data that was analyzed previously by 70 teams (NARPS; Botvinik-Nezer, et al., 2020), showing that it is possible to balance the goals of displaying a full set of results with providing the reader reasonably concise and “digestible” findings. In particular, the highlighting approach sheds useful light on the kind of variability present among the NARPS teams’ results, which is primarily a varied strength of agreement rather than disagreement. Using a meta-analysis built on the informative “highlighting” approach shows this relative agreement, while one using the standard “hiding” approach does not. We describe how this simple but powerful change in practice—focusing on highlighting results, rather than hiding all but the strongest ones—can help address many large concerns within the field, or at least to provide more complete information about them. We include a list of practical suggestions for results reporting to improve reproducibility, cross-study comparisons and meta-analyses.]]></summary></entry><entry xml:lang="english"><title type="html">Replication of the natural selection of bad science</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/20/Replication-of-bad-science.html" rel="alternate" type="text/html" title="Replication of the natural selection of bad science" /><published>2022-10-20T07:00:00+00:00</published><updated>2022-10-20T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/20/Replication-of-bad-science</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/10/20/Replication-of-bad-science.html"><![CDATA[<p>This study reports an independent replication of the findings presented by Smaldino and McElreath (2016), “The natural selection of bad science”. The replication was successful with one exception. We find that selection acting on scientist’s propensity for replication frequency caused a brief period of exuberant replication not observed in the original paper due to a coding error. This difference does not, however, change the authors’ original conclusions. We call for more replication studies for simulations as unique contributions to scientific quality assurance.</p>

<p><a href="https://osf.io/preprints/metaarxiv/sjyp3/"><span id="Kohrt2022"><span style="font-variant: small-caps">Kohrt, F., Smaldino, P.E., McElreath, R., and Schönbrodt, F.D.</span> 2022. Replication of the natural selection of bad science. .</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="anthropology" /><summary type="html"><![CDATA[This study reports an independent replication of the findings presented by Smaldino and McElreath (2016), “The natural selection of bad science”. The replication was successful with one exception. We find that selection acting on scientist’s propensity for replication frequency caused a brief period of exuberant replication not observed in the original paper due to a coding error. This difference does not, however, change the authors’ original conclusions. We call for more replication studies for simulations as unique contributions to scientific quality assurance.]]></summary></entry></feed>