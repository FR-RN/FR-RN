<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://www.recherche-reproductible.fr/FR-RN/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.recherche-reproductible.fr/FR-RN/" rel="alternate" type="text/html" /><updated>2023-03-17T19:24:21+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/feed.xml</id><title type="html">Recherche Reproductible</title><subtitle>Un réseau national de chercheurs pour la promotion de la recherche reproductible.</subtitle><author><name>Recherche Reproductible</name></author><entry><title type="html">GT Notebook Webinaire #1 Quarto</title><link href="https://www.recherche-reproductible.fr/FR-RN/news/2023/03/30/GT-Notebook-March-Webinaire.html" rel="alternate" type="text/html" title="GT Notebook Webinaire #1 Quarto" /><published>2023-03-30T12:30:00+00:00</published><updated>2023-03-30T12:30:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/news/2023/03/30/GT-Notebook-March-Webinaire</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/news/2023/03/30/GT-Notebook-March-Webinaire.html"><![CDATA[<p>Qu’on les appelle “notebooks”, “article exécutable” ou “publication reproductible”, la démocratisation du <em>Literate Programming</em> questionne non seulement nos pratiques scientifiques mais également les chaînes de publications et leurs infrastructures matérielles et logicielles, tout comme leur appropriation et les médiations entre équipes de recherche, étudiants et étudiantes et personnel d’appui à la recherche.</p>

<p>Le gt notebook est déjà composé d’une vingtaine de membres venant de toute discipline. Il travaille sur plusieurs plateformes : une <a href="https://gitlab.huma-num.fr/gt-notebook">forge gitlab</a>, un <a href="https://gt-notebook.gitpages.huma-num.fr/site_quarto/">espace documentaire Quarto [<em>work in progress</em>]</a> et <a href="https://groupes.renater.fr/sympa/info/notebooks-inter-reseaux">une liste de diffusion</a> sont ouverts à toute personne de toute discipline intéressée par les échanges et les pratiques sur ces questions.</p>

<p>Dans le cadre de la première année de son webinaire, le gt notebook vous invite à <a href="https://gt-notebook.gitpages.huma-num.fr/site_quarto/posts/webinaire1.html">échanger autour de la technologie <strong>Quarto</strong></a> présentée par Nicolas Roelandt, ingénieur géomaticien au sein du département Aménagement, Mobilités, Environnement de l’Université Gustave Eiffel,<br />
<strong>le jeudi 30 mars de 13h30 à 14h30</strong> 
<a href="https://enquetes.univ-rouen.fr/861446?lang=fr">S’inscrire</a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="news" /><summary type="html"><![CDATA[Qu’on les appelle “notebooks”, “article exécutable” ou “publication reproductible”, la démocratisation du Literate Programming questionne non seulement nos pratiques scientifiques mais également les chaînes de publications et leurs infrastructures matérielles et logicielles, tout comme leur appropriation et les médiations entre équipes de recherche, étudiants et étudiantes et personnel d’appui à la recherche.]]></summary></entry><entry xml:lang="english"><title type="html">An implementation framework to improve the transparency and reproducibility of computational models of infectious diseases</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2023/03/17/infectious-diseases.org" rel="alternate" type="text/html" title="An implementation framework to improve the transparency and reproducibility of computational models of infectious diseases" /><published>2023-03-17T19:00:00+00:00</published><updated>2023-03-17T19:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2023/03/17/infectious-diseases</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2023/03/17/infectious-diseases.org"><![CDATA[Computational models of infectious diseases have become valuable tools for research and the public health response against epidemic threats. The reproducibility of computational models has been limited, undermining the scientific process and possibly trust in modeling results and related response strategies, such as vaccination. We translated published reproducibility guidelines from a wide range of scientific disciplines into an implementation framework for improving reproducibility of infectious disease computational models. The framework comprises 22 elements that should be described, grouped into 6 categories: computational environment, analytical software, model description, model implementation, data, and experimental protocol. The framework can be used by scientific communities to develop actionable tools for sharing computational models in a reproducible way.
    
[<span id="Pokutnaya:2023"><span style="font-variant: small-caps">Pokutnaya, D., Childers, B., Arcury-Quandt, A.E., Hochheiser, H., and Van Panhuis, W.G.</span> 2023. An implementation framework to improve the transparency and reproducibility of computational models of infectious diseases. <i>PLOS Computational Biology</i> <i>19</i>, 3, 1–9.</span>](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010856)]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="science" /><summary type="html"><![CDATA[Computational models of infectious diseases have become valuable tools for research and the public health response against epidemic threats. The reproducibility of computational models has been limited, undermining the scientific process and possibly trust in modeling results and related response strategies, such as vaccination. We translated published reproducibility guidelines from a wide range of scientific disciplines into an implementation framework for improving reproducibility of infectious disease computational models. The framework comprises 22 elements that should be described, grouped into 6 categories: computational environment, analytical software, model description, model implementation, data, and experimental protocol. The framework can be used by scientific communities to develop actionable tools for sharing computational models in a reproducible way. [Pokutnaya, D., Childers, B., Arcury-Quandt, A.E., Hochheiser, H., and Van Panhuis, W.G. 2023. An implementation framework to improve the transparency and reproducibility of computational models of infectious diseases. PLOS Computational Biology 19, 3, 1–9.](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010856)]]></summary></entry><entry xml:lang="english"><title type="html">Ten simple rules for designing and conducting undergraduate replication projects</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2023/03/16/ten-rules.html" rel="alternate" type="text/html" title="Ten simple rules for designing and conducting undergraduate replication projects" /><published>2023-03-16T19:00:00+00:00</published><updated>2023-03-16T19:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2023/03/16/ten-rules</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2023/03/16/ten-rules.html"><![CDATA[<p>Conducting a replication study is a valuable way for undergraduate students to learn about the scientific process and gain research experience. By promoting the evaluation of existing studies to confirm their reliability, replications play a unique, though often underappreciated, role in the scientific enterprise. Involving students early in this process can help make replication mainstream among the new generation of scientists. Beyond their benefit to science, replications also provide an invaluable learning ground for students, from encouraging the development of critical thinking to emphasizing the importance of details and honing research skills. In this piece, we outline 10 simple rules for designing and conducting undergraduate replication projects, from conceptualization to implementation and dissemination. We hope that these guidelines can help educators provide students with a meaningful and constructive pedagogical experience, without compromising the scientific value of the replication project, therefore ensuring robust, valuable contributions to our understanding of the world.</p>

<p><a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1010957"><span id="Moreau:2023"><span style="font-variant: small-caps">Moreau, D. and Wiebels, K.</span> 2023. Ten simple rules for designing and conducting undergraduate replication projects. <i>PLOS Computational Biology</i> <i>19</i>, 3, 1–11.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="science" /><summary type="html"><![CDATA[Conducting a replication study is a valuable way for undergraduate students to learn about the scientific process and gain research experience. By promoting the evaluation of existing studies to confirm their reliability, replications play a unique, though often underappreciated, role in the scientific enterprise. Involving students early in this process can help make replication mainstream among the new generation of scientists. Beyond their benefit to science, replications also provide an invaluable learning ground for students, from encouraging the development of critical thinking to emphasizing the importance of details and honing research skills. In this piece, we outline 10 simple rules for designing and conducting undergraduate replication projects, from conceptualization to implementation and dissemination. We hope that these guidelines can help educators provide students with a meaningful and constructive pedagogical experience, without compromising the scientific value of the replication project, therefore ensuring robust, valuable contributions to our understanding of the world.]]></summary></entry><entry><title type="html">Recherche Reproductible: états des lieux (compte-rendu)</title><link href="https://www.recherche-reproductible.fr/FR-RN/news/2023/03/15/RR-days-summary.html" rel="alternate" type="text/html" title="Recherche Reproductible: états des lieux (compte-rendu)" /><published>2023-03-15T07:00:00+00:00</published><updated>2023-03-15T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/news/2023/03/15/RR-days-summary</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/news/2023/03/15/RR-days-summary.html"><![CDATA[<p>Les premières journées du <a href="http://www.recherche-reproductible.fr/">réseau français de la recherche reproductible</a> se sont déroulées à l’Institut Pasteur les 8 et 9 mars 2023, en mode hybride. L’objectif de ces journées était de dresser un état des lieux de la reproductibilité dans les domaines de la recherche scientifique en France, en tenant compte de la diversité des disciplines et des pratiques. Les 2 jours ont été divisés en 6 sessions, contenant des présentations de 10 minutes, suivies à chaque fois d’une discussion générale. <strong><a href="/assets/rrdays/00-Introduction.pdf">[slides]</a></strong></p>

<h2 id="1-reproductibilité-observationnelle">1. Reproductibilité observationnelle</h2>

<p>L’objectif de cette session était d’identifier les sources de non-reproductibilité dans le domaine observationnel, les solutions envisagées, et les nouvelles questions qui en émergent.</p>

<p><strong>Alexandre Hocquet</strong> a ouvert les journées en nous rappelant que, depuis les années 1980, les historiens des sciences travaillent sur la thématique de la reproductibilité en science, mais que la communauté scientifique ne s’y intéresse que depuis la crise de la reproductibilité. Il a cité les recherches de Sabina Leonelli, qui permettent de mieux cerner les multiples facettes de la reproductibilité <em>et offre une catégorisation de ces facettes</em>. La reproductibilité s’envisage de multiples façons, y compris à l’intérieur du même domaine de recherche.</p>

<p><strong>Timothée Giraud</strong> est ensuite intervenu pour expliquer le point de vue des sciences humaines et sociales, où les traitements numériques sont le quotidien. Les problèmes sont l’utilisation d’outils numériques avec interface graphique, ne permettant pas une bonne traçabilité des étapes des analyses, et l’utilisation de logiciels privateurs qui entrainent des problèmes de formats et de transparence des méthodes d’analyse. Les formations initiales abordent peu les outils et notions de la recherche reproductible, et les personnes se forment par elles-mêmes, sans acquérir les bonnes pratiques de programmation.
<strong><a href="/assets/rrdays/02-Giraud.pdf">[slides]</a></strong></p>

<p><strong>Thomas Vuillaume</strong> a présenté <a href="https://projectescape.eu/">le projet européen ESCAPE</a> (cluster pour la physique des particules et l’astronomie) en mettant notamment en avant les <a href="https://www.ouvrirlascience.fr/fair-principles/">données FAIR</a>, les <a href="https://www.nature.com/articles/s41597-022-01710-x">logiciels FAIR</a> et <a href="https://projectescape.eu/services">les moyens techniques</a> pour une recherche reproductible dans les grandes infrastructures de recherche partenaires d’ESCAPE. Il a expliqué que ces moyens techniques ne suffisent pas et que les chercheurs aussi doivent être “FAIR” (c’est-à-dire qui adoptent et appliquent les principes FAIR) et que la reproductibilité ne doit pas être un objectif dès le départ, mais qu’il faut y tendre progressivement, par étapes. <strong><a href="/assets/rrdays/03-Vuillaume.pdf">[slides]</a></strong></p>

<h2 id="2-reproductibilité-computationnelle">2. Reproductibilité computationnelle</h2>

<p>L’objectif de cette session était d’identifier les sources de non-reproductibilité dans le domaine computationnel, les solutions envisagées, et les nouvelles questions qui en émergent.</p>

<p>À partir d’un exemple en biophysique moléculaire, <strong>Konrad Hinsen</strong> a montré les limites de la reproductibilité computationelle, nous rappelant que la confusion entre un modèle scientifique (défini) et son implémentation informatique (variable) condamne presque tout effort de réplicabilité, et que la négligence dans la gestion des environnement logiciel empêche la reproductibilité. On a fait du progrès au cours des dernières années, mais il reste beaucoup de chemin à faire. <strong><a href="/assets/rrdays/04-Hinsen.pdf">[slides]</a></strong></p>

<p><strong>Roberto Di Cosmo</strong> a poursuivi en faisant un historique de la prise de conscience des problèmes de reproductibilité en informatique et les efforts faits pour y pallier. La Digital Library de l’<a href="https://dl.acm.org/journals">ACM</a> s’efforce de lier article et code, mais le code source n’est pas toujours publié avec l’article, et quand il l’est, il manque l’historique de dévéloppement, essentiel pour le comprendre et le ré-utiliser. La communauté du machine learning est à l’origine de <em>Papers with Code</em>, mais le lecteur est renvoyé directement à la plateforme de développement du code source, ce qui ne permet pas d’identifier la version précise utilisée dans l’article et ne garantit pas un accès pérenne. Une solution existe pour préserver sur le long terme les codes sources et identifier précisément les versions associées aux publications : <a href="https://archive.softwareheritage.org/">Software Heritage</a>. Cette méthode permet notamment la conservation du code source, le suivi des versions et un référence plus précis et sûr que des simples DOI. Elle est adoptées par des journaux comme IPOL, eLife ou JTCAM.</p>

<p><strong>Camille Maumet</strong> nous a présenté les problèmes de reproductibilité en neuro-imagerie, notamment autour de l’analyse des données  - qui même si elle est automatique est soumise à de nombreux choix réalisés par des experts - et donc difficilement reproductible. Un autre problème est la taille de l’études, qui n’est pas toujours suffisante pour conférer une puissance statistique adéquate. Pour y remédier, des consortiums sont mis en place afin de mutualiser les données. Elle a aussi mentionné le nombre d’étapes dans les analyses, entre données brutes et résultats finaux, et la nécessité de travailler de façon collaborative au sein de la communauté pour mettre en place de bonnes pratiques.
<strong><a href="/assets/rrdays/06-Maumet.pdf">[slides]</a></strong></p>

<p><strong>Thomas Moreau</strong> a présenté <a href="https://benchopt.github.io/">l’environnement BenchOpt</a>, développé dans le cadre de l’apprentissage automatique. BenchOpt permet d’assurer la création de benchmarks collaboratifs et efficaces, en combinant plusieurs langages dans un même benchmark. BenchOpt permet de comparer et de comprendre la variabilité de traitements de données d’une méthode à l’autre. Le but de benchopt n’est pas d’assurer une reproducibilité bit à bit, mais plutot une reproducibilité statistique des resultats, tout en promouvant une forme de maintenance des benchmarks, pour que les resultats soit mis à jour avec les évolutions des softwares. <strong><a href="/assets/rrdays/07-Moreau.pdf">[slides]</a></strong></p>

<h2 id="3-reproductibilité-statistique">3. Reproductibilité statistique</h2>

<p>L’objectif de cette session était d’identifier les sources de non-reproductibilité dans le domaine des méthodes statistiques, les solutions envisagées, et les nouvelles questions qui en émergent.</p>

<p>Après avoir rappelé le très faible taux de reproductibilité en économie, <strong>Christophe Hurlin</strong> a présenté <a href="https://www.ouvrirlascience.fr/cascad-la-certification-de-la-reproductibilite-de-la-recherche-scientifique/">Cascad</a>, une UMS de certification de codes et de données qui permet de garantir la reproductibilité des articles. Cette UMS est aujourd’hui sollicitée par des journaux en économie et a déjà certifié une cinquantaine de codes.</p>

<p><strong>Nelle Varoquaux</strong> a ensuite présenté les problématiques rencontrées en bio-physique, biochimie et évolution où les facteurs de non-reproductibilité sont nombreux : origine des données, annotations des génomes, méthode d’analyses, disponibilité et accessibilité du code, temps de calcul parfois long qui empêche les ré-analyses. La nouvelle revue <a href="http://journal-sfds.fr/">SFDS</a> tente d’y remédier en publiant des analyses entièrement reproductibles, et ce dans un temps raisonable.</p>

<p>Pour <strong>Isabelle Boutron</strong>, la transparence pour les essais cliniques vient avant même la question de la reproductibilité de ces essais. Sous l’impulsion de Drumond Rennie, le pré-enregistrement des protocoles des essais ainsi que des directives claires pour écrire les rapports sont devenus incontournables, notamment pour éviter le “harking” ou post-hoc analysis.</p>

<h2 id="4-reproductibilité-expérimentale">4. Reproductibilité expérimentale</h2>

<p>L’objectif de cette session était d’identifier les sources de non-reproductibilité en recherche expérimentale, les solutions envisagées, et les nouvelles questions qui en émergent.</p>

<p><strong>Fay Betsou</strong> a commencé son intervention en soulignant qu’il existe d’innombrables facteurs de variabilité dans la préparation des échantillons biologiques, avant même de réaliser des expériences à la paillasse. Ces facteurs sont rarement, ou mal, mentionnés dans les méthodes d’un article. Il est donc nécesaire d’avoir de nouveaux standards de rédaction de protocole et la traçabilité des données.</p>

<p><strong>Florian Naudet</strong> nous a rappelé le cycle hypothetico-déductif et les déviances, telles que le <em>harking</em> ou le <em>p-hacking</em>. A travers différents exemples, il a insisté sur les conséquences de santé publique des difficultés à reproduire la recherche thérapeutique (e.g. rTMS, antidépresseurs, traitement du paludisme). Les solutions envisagées reposent sur une meilleure conception et l’enregistrement des études, le partage des données, la nécessité d’aligner les incitatifs et de former une nouvelle génération de chercheurs à ces enjeux. Il est important de mettre en oeuvre une approche de métarecherche permettant d’évaluer l’impact de ces solutions.</p>

<p><strong>François Ric</strong> a présenté un bref historique de la reproductibilité en psychologie et comment la communauté s’est saisie du problème pour se réformer en profondeur, notamment via l’<a href="https://osf.io/">Open Science Framework</a>. Aujourd’hui, le pré-enregistrement des études, le partage des données et la transparence des méthodes ont été adoptés. <strong><a href="/assets/rrdays/12-Ric.pdf">[slides]</a></strong></p>

<p><strong>Yoann Lafon</strong> a présenté les problèmes de reproductibilité méthodologique en caractérisation de matériaux biologiques, poussée notamment par les nouvelles réglementations pour la mise sur le marché des dispositifs médicaux autorisant le recours à la simulation numérique. Se posent alors le problème de la certification des données utilisées dans ces simulations. Un consortium international a donc rassemblé 25 équipes pour améliorer la reproductibilité de la caractérisation mécanique de tissus biologiques. Les membres du consortium ont travaillé sur les mêmes données, d’abord avec des protocoles différents, puis en appliquant un protocole unique issu d’un consensus. des travaux sont encore en cours pour réduire les incertitudes expérimentales. L’effet de ces incertitudes sur le comportement des modèles dépend finalement des paramètres de sortie utilisés pour répondre à la Question d’Intérêt.</p>

<h2 id="5-formation-et-enseignement">5. Formation et enseignement</h2>

<p>La sensibilisation aux notions de reproductibilité dans le domaine de la recherche peut être effectuée en formation initiale, dès le master, ou dans le cadre professionnel. L’objectif de cette session a été de présenter des exemples actuellement mis en place dans certains masters, et des solutions de formation dans le milieu professionnel.</p>

<p><strong>Frédéric Lemoine</strong> a présenté le <em>Reprohackathon</em>, un module d’enseignement mis en place depuis 2020 dans le master AMI2B (Université Paris-Saclay, ~40 étudiants/an) sur la reproductibilité computationnelle. Ce module est composé d’une partie théorique+pratique et d’un projet long, consistant pour les étudiants en un travail de plusieurs mois en semi-autonomie. Les étudiants doivent comprendre la méthode utilisée dans un article pour ensuite la mettre en oeuvre via un workflow afin d’en reproduire les résultats. L’objectif est de comprendre l’intérêt de la reproductibilité et d’en connaître en profondeur les concepts techniques et outils.
<strong><a href="/assets/rrdays/14-Lemoine.pdf">[slides]</a></strong></p>

<p><a href="https://reproducibilitea.org/">ReproducibiliTea</a> est une initiative anglaise reprise <a href="https://www.bordeaux-neurocampus.fr/reproducibilitea-in-bordeaux-another-successful-year/">à Bordeaux par <strong>Eduarda Centeno</strong> et <strong>Fjola Hyseni</strong></a>. Elles nous ont présenté sa mise en place en 2021 ainsi que son organisation. Après deux ans, elles ont organisé 18 sessions sur une variété de sujets sur la science ouverte et la reproductibilité, y compris des sessions spéciales avec des invités de l’UNESCO, FORRT et d’autres ‘ReproductibiliTea’s. Le BordeauxTea est devenu une formation doctorale sur l’ADUM, donnant des heures et des certificats aux étudiants participants.
<strong><a href="/assets/rrdays/15-Centeno-Hyseni.pdf">[slides]</a></strong></p>

<p><strong>Laurent Oudre</strong> nous a ensuite présenté un nouveau track d’enseignement dans le master MVA (ENS Paris-Saclay) composé d’un cours théorique et d’un cours pratique. Le but pour les étudiants est de pouvoir soumettre un article à la <a href="https://www.ipol.im/">revue IPOL</a>, très exigeante sur les aspects de reproductibilité.  <strong><a href="/assets/rrdays/17-Oudre.pdf">[slides]</a></strong></p>

<p><strong>Arnaud Legrand</strong> a terminé la session en expliquant la genèse du MOOC “Recherche Reproductible”, qui compte désormais plus de 13 000 participants depuis la troisième édition ouverte en 2020. Une suite de ce MOOC, plus technique, est appelée à se développer avec 3 modules en préparation : gestion des données, contrôle des environnements logiciels et workflow scientifique. Ces modules seront disponibles à la fin de l’automne 2023. Ce retour d’expérience permet aussi de mettre en lumière la problématique de la sensibilisation de communautés variées : comment toucher efficacement des utilisateurs de domaines pouvant se percevoir éloignés de ces questions ?</p>

<h2 id="6-europe-et-international">6. Europe et international</h2>

<p>Dans de nombreux pays, il existe des réseaux académiques nationaux dédiés à la question de la reproductibilité. Un des enjeux de ces journées a été de réfléchir à la définition et à la gouvernance d’un tel réseau, à l’échelle nationale française.</p>

<p><strong>Étienne Roesch</strong> a présenté le United Kingdom Reproducitibility Network (<a href="https://www.ukrn.org/">UKRN</a>) fondé en 2018 en abordant son organisation, ses financements et son développement. Ce réseau, aujourd’hui composé de 70 noeuds locaux, 25 universités et 50 “stakeholders”, permet d’influer sur les politiques. Toutefois, Étienne met en garde contre les risques de dérives : le <em>reproducibility washing</em>. <strong><a href="/assets/rrdays/19-Roesch.pdf">[slides]</a></strong></p>

<p><strong>Vittorio Iacovella</strong> est le fondateur du réseau italien de reproductibilité (<a href="https://www.itrn.org/">ITRN</a>) qui a démarré en mars 2021, à la suite de discussions datant de 2017. Le réseau se veut un lieu de formation, de discussion, d’organisation. Le réseau est aussi conçu pour permettre la formation de groupes de travail et est toujours en évolution.</p>

<p><strong>Isabelle Blanc</strong> a terminé la session en présentant le Comité pour la Science Ouverte (<a href="https://www.ouvrirlascience.fr/accueil/">COSO</a>) en expliquant notamment ses ambitions, son organisation, ses réalisations et son influence sur le paysage scientifique français, européen et international. Le COSO pourrait être une structure d’accueil pour un futur réseau national français de la recherche reproductible.</p>

<h2 id="discussion-générale">Discussion générale</h2>

<p>Ces deux journées se sont terminées par une discussion générale concernant l’avenir de la communauté de recherche reproductible en France.  L’objectif de ces premières journées était de réaliser un état des lieux de la reproductibilité dans divers domaines de la recherche scientifique. Ces journées ont permis aux personnes intéressées de se réunir afin d’échanger sur les objectifs de créer une initiative nationale autours de la recherche reproductible, et les prochaines étapes. Outre le sujet des financements, la question des fonctions support dont le réseau a besoin pour se développer et se structurer a été notamment abordée et va être examinée.</p>

<p>Les personnes présentes (physiquement ou en ligne) semblent enthousiastes à l’idée de se structurer. Une réunion en ligne sera bientôt organisée afin de poursuivre nos efforts. Un questionnaire va être diffusé afin de recenser, à l’échelle nationale, les initiatives locales de reproductibilité ; et de savoir quelles personnes seraient volontaires pour s’impliquer dans la mise en place d’un projet d’envergure nationale, et dans quelle mesure.</p>]]></content><author><name>Recherche Reproductible</name></author><category term="news" /><summary type="html"><![CDATA[Les premières journées du réseau français de la recherche reproductible se sont déroulées à l’Institut Pasteur les 8 et 9 mars 2023, en mode hybride. L’objectif de ces journées était de dresser un état des lieux de la reproductibilité dans les domaines de la recherche scientifique en France, en tenant compte de la diversité des disciplines et des pratiques. Les 2 jours ont été divisés en 6 sessions, contenant des présentations de 10 minutes, suivies à chaque fois d’une discussion générale. [slides]]]></summary></entry><entry><title type="html">Recherche Reproductible: états des lieux</title><link href="https://www.recherche-reproductible.fr/FR-RN/past-event/2023/03/08/RR-Days.html" rel="alternate" type="text/html" title="Recherche Reproductible: états des lieux" /><published>2023-03-08T13:00:00+00:00</published><updated>2023-03-08T13:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/past-event/2023/03/08/RR-Days</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/past-event/2023/03/08/RR-Days.html"><![CDATA[<p>Un nombre croissant de chercheurs s’intéresse à la problématique de la reproductibilité, dont la définition même peut grandement varier d’une discpline à une autre (observationnelle, expérimentale, statistique, computationnelle, …). Or, nous avons rarement l’occasion de poser un regard inter-disciplinaire sur nos approches et définitions respectives. Ce <a href="http://www.recherche-reproductible.fr/rr-days/">workshop</a> se veut donc un lieu d’échanges et d’information pour dresser un premier état des lieux de la reproductibilité en France. Le but n’est donc pas tant de faire de longs exposés, mais plutôt de favoriser les interventions courtes suivies de discussion permettant à tout un chacun de mieux comprendre les problématiques des autres en général et de la jeune génération (doctorants et post-doctorants) en particulier. Nous essayons donc de rassembler des scientifiques de toutes disciplines.</p>

<p>Plus d’informations sur: <a href="http://www.recherche-reproductible.fr/rr-days">www.recherche-reproductible.fr/rr-days</a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="past-event" /><summary type="html"><![CDATA[Un nombre croissant de chercheurs s’intéresse à la problématique de la reproductibilité, dont la définition même peut grandement varier d’une discpline à une autre (observationnelle, expérimentale, statistique, computationnelle, …). Or, nous avons rarement l’occasion de poser un regard inter-disciplinaire sur nos approches et définitions respectives. Ce workshop se veut donc un lieu d’échanges et d’information pour dresser un premier état des lieux de la reproductibilité en France. Le but n’est donc pas tant de faire de longs exposés, mais plutôt de favoriser les interventions courtes suivies de discussion permettant à tout un chacun de mieux comprendre les problématiques des autres en général et de la jeune génération (doctorants et post-doctorants) en particulier. Nous essayons donc de rassembler des scientifiques de toutes disciplines.]]></summary></entry><entry xml:lang="english"><title type="html">The reproducibility issues that haunt health-care AI</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/13/Health-AI.html" rel="alternate" type="text/html" title="The reproducibility issues that haunt health-care AI" /><published>2023-01-13T08:00:00+00:00</published><updated>2023-01-13T08:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/13/Health-AI</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/13/Health-AI.html"><![CDATA[<p>Health-care systems are rolling out artificial-intelligence tools for
diagnosis and monitoring. But how reliable are the models?</p>

<p>Each day, around 350 people in the United States die from lung cancer. Many of those deaths could be prevented by screening with low-dose computed tomography (CT) scans. But scanning millions of people would produce millions of images, and there aren’t enough radiologists to do the work. Even if there were, specialists regularly disagree about whether images show cancer or not. The 2017 Kaggle Data Science Bowl set out to test whether machine-learning algorithms could fill the gap.</p>

<p><a href="https://www.nature.com/articles/d41586-023-00023-2"><span id="Sohn:2023"><span style="font-variant: small-caps">Sohn, E.</span> 2023. The reproducibility issues that haunt health-care AI. <i>Nature</i> <i>613</i>, 7943, 402–403.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="medicine," /><category term="artificial" /><category term="intelligence" /><summary type="html"><![CDATA[Health-care systems are rolling out artificial-intelligence tools for diagnosis and monitoring. But how reliable are the models?]]></summary></entry><entry><title type="html">Graphics Replicability Stamp Initiative (GRSI)</title><link href="https://www.recherche-reproductible.fr/FR-RN/news/2023/01/13/Graphics-Replicability-Stamps.html" rel="alternate" type="text/html" title="Graphics Replicability Stamp Initiative (GRSI)" /><published>2023-01-13T07:00:00+00:00</published><updated>2023-01-13T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/news/2023/01/13/Graphics-Replicability-Stamps</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/news/2023/01/13/Graphics-Replicability-Stamps.html"><![CDATA[<p>The <a href="http://www.replicabilitystamp.org/">Graphics Replicability Stamp Initiative (GRSI)</a> is an independent group of volunteers who help the community by enabling sharing of code and data as a community resource for non-commercial use. The volunteers check the submitted code and certify its replicability, awarding a replicability stamp, which is an additional recognition for authors of accepted papers who are willing to provide a complete implementation of their algorithm, to replicate the results presented in their paper. The replicability stamp is not meant to be a measure of the scientiﬁc quality of the paper or of the usefulness of presented algorithms. Rather, it is meant to be an endorsement of the replicability of the results presented in the paper and the recognition of the service provided to the community by releasing the code. Submissions for the replicability stamp will be considered only <em>after</em> the paper has been fully accepted. Submissions that are awarded the replicability stamp will receive additional exposure by being listed on this website. The purpose of this stamp is to promote reproducibility of research results and to allow scientists and practitioners to immediately beneﬁt from state-of-the-art research results, without spending months re-implementing the proposed algorithms and trying to ﬁnd the right parameter values. We also hope that it will indirectly foster scientiﬁc progress, since it will allow researchers to reliably compare with and to build upon existing techniques, knowing that they are using exactly the same implementation.</p>]]></content><author><name>Recherche Reproductible</name></author><category term="news" /><summary type="html"><![CDATA[The Graphics Replicability Stamp Initiative (GRSI) is an independent group of volunteers who help the community by enabling sharing of code and data as a community resource for non-commercial use. The volunteers check the submitted code and certify its replicability, awarding a replicability stamp, which is an additional recognition for authors of accepted papers who are willing to provide a complete implementation of their algorithm, to replicate the results presented in their paper. The replicability stamp is not meant to be a measure of the scientiﬁc quality of the paper or of the usefulness of presented algorithms. Rather, it is meant to be an endorsement of the replicability of the results presented in the paper and the recognition of the service provided to the community by releasing the code. Submissions for the replicability stamp will be considered only after the paper has been fully accepted. Submissions that are awarded the replicability stamp will receive additional exposure by being listed on this website. The purpose of this stamp is to promote reproducibility of research results and to allow scientists and practitioners to immediately beneﬁt from state-of-the-art research results, without spending months re-implementing the proposed algorithms and trying to ﬁnd the right parameter values. We also hope that it will indirectly foster scientiﬁc progress, since it will allow researchers to reliably compare with and to build upon existing techniques, knowing that they are using exactly the same implementation.]]></summary></entry><entry><title type="html">Workshop on Reproducibility in Cancer Biology [en]</title><link href="https://www.recherche-reproductible.fr/FR-RN/past-event/2023/01/11/Cancer-Biology.html" rel="alternate" type="text/html" title="Workshop on Reproducibility in Cancer Biology [en]" /><published>2023-01-11T08:00:00+00:00</published><updated>2023-01-11T08:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/past-event/2023/01/11/Cancer-Biology</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/past-event/2023/01/11/Cancer-Biology.html"><![CDATA[<p>The lack of reproducibility in life sciences research, and particularly in Cancer Biology, is a growing concern in recent years, often referred to as ‘Reproducibility Crisis’. The Center for Open Science published in 2021, in the journal eLife, the results of its ‘Reproducibility Project: Cancer Biology’ reporting poor reproducibility of previously published results in the field of Cancer Research.</p>

<p><a href="https://oncospheremeeting.com/workshop-on-reproducibility-in-cancer-biology/">In this
workshop</a>
experts in different fields, through a transdisciplinary approach,
will discuss the causes, the implications and potential solutions for
the Reproducibility problems in Cancer Research.</p>

<p>More information at https://oncospheremeeting.com/workshop-on-reproducibility-in-cancer-biology/</p>]]></content><author><name>Recherche Reproductible</name></author><category term="past-event" /><summary type="html"><![CDATA[The lack of reproducibility in life sciences research, and particularly in Cancer Biology, is a growing concern in recent years, often referred to as ‘Reproducibility Crisis’. The Center for Open Science published in 2021, in the journal eLife, the results of its ‘Reproducibility Project: Cancer Biology’ reporting poor reproducibility of previously published results in the field of Cancer Research.]]></summary></entry><entry xml:lang="english"><title type="html">Benchopt: Reproducible, efficient and collaborative optimization benchmarks</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/09/Benchopt.html" rel="alternate" type="text/html" title="Benchopt: Reproducible, efficient and collaborative optimization benchmarks" /><published>2023-01-09T07:00:00+00:00</published><updated>2023-01-09T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/09/Benchopt</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/09/Benchopt.html"><![CDATA[<p>Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks:
L2-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details.</p>

<p><a href="https://openreview.net/forum?id=1uSzacpyWLH"><span id="Moreau:2022"><span style="font-variant: small-caps">Moreau, T., Massias, M., Gramfort, A., et al.</span> 2022. Benchopt: Reproducible, efficient and collaborative optimization benchmarks. <i>Advances in Neural Information Processing Systems</i>.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="computational" /><category term="science" /><summary type="html"><![CDATA[Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks: L2-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details.]]></summary></entry><entry xml:lang="english"><title type="html">A Student’s Guide to Open Science: Using the Replication Crisis to Reform Psychology</title><link href="https://www.recherche-reproductible.fr/FR-RN/publication/2022/12/31/student-guide-open-science.html" rel="alternate" type="text/html" title="A Student’s Guide to Open Science: Using the Replication Crisis to Reform Psychology" /><published>2022-12-31T07:00:00+00:00</published><updated>2022-12-31T07:00:00+00:00</updated><id>https://www.recherche-reproductible.fr/FR-RN/publication/2022/12/31/student-guide-open-science</id><content type="html" xml:base="https://www.recherche-reproductible.fr/FR-RN/publication/2022/12/31/student-guide-open-science.html"><![CDATA[<p>A Student’s Guide to Open Science explores the so-called “replication crisis” in psychology (the inherent difficulties in reproducing research results to test the robustness of findings) while delving into the ways Open Science can address the crisis by transforming research practice.</p>

<p>Students will develop a fundamental understanding of the drivers and origins of the crisis and learn how Open Science practices can enhance research transparency, replication, and reproducibility.</p>

<p>With a handy, digestible guide for students and researchers alike on how to implement Open Science practices within their own workflow, as well as pedagogic teaching and learning activities that can be re-used by educators, Pennington’s new book is an essential guide to navigating the replication crisis.</p>

<p>Key features of this book include:</p>

<ul>
  <li>Case studies of classic psychological studies undergoing replication. </li>
  <li>Test yourself activities to reinforce learning of key concepts, including an open science crossword!</li>
  <li>Top tips boxes for open science practices including preregistration, Registered Reports, and open materials, code, and data. </li>
  <li>Useful illustrations to aid understanding and facilitate revision.</li>
</ul>

<p>New concepts and practices can often feel overwhelming, but this book aims to help students to pick what they want from the Open Science buffet and encourages them to keep returning to the table to fill up their plates again and again. Remember, we are all students of open science and will be for many years to come!</p>

<p>Charlotte Pennington is a Lecturer in Psychology at Aston University, UK and a Fellow of the Higher Education Academy. She is an expert in open science and advocates for the teaching of this within higher education pedagogy. </p>

<p><a href="https://www.amazon.co.uk/Students-Guide-Open-Science-Replication/dp/0335251161"><span id="Pennington2023"><span style="font-variant: small-caps">Pennington, C.</span> 2023. <i>A Student’s Guide to Open Science: Using the Replication Crisis to Reform Psychology</i>. Amazon.</span></a></p>]]></content><author><name>Recherche Reproductible</name></author><category term="publication" /><category term="psychology" /><summary type="html"><![CDATA[A Student’s Guide to Open Science explores the so-called “replication crisis” in psychology (the inherent difficulties in reproducing research results to test the robustness of findings) while delving into the ways Open Science can address the crisis by transforming research practice.]]></summary></entry></feed>