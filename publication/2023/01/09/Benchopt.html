<!DOCTYPE html>
<html lang="english"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Benchopt: Reproducible, efficient and collaborative optimization benchmarks | Recherche Reproductible</title>
<meta name="generator" content="Jekyll v4.2.2" />
<meta property="og:title" content="Benchopt: Reproducible, efficient and collaborative optimization benchmarks" />
<meta name="author" content="Recherche Reproductible" />
<meta property="og:locale" content="english" />
<meta name="description" content="Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks: L2-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details." />
<meta property="og:description" content="Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks: L2-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details." />
<link rel="canonical" href="https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/09/Benchopt.html" />
<meta property="og:url" content="https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/09/Benchopt.html" />
<meta property="og:site_name" content="Recherche Reproductible" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-01-09T07:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Benchopt: Reproducible, efficient and collaborative optimization benchmarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Recherche Reproductible"},"dateModified":"2023-01-09T07:00:00+00:00","datePublished":"2023-01-09T07:00:00+00:00","description":"Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks: L2-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details.","headline":"Benchopt: Reproducible, efficient and collaborative optimization benchmarks","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/09/Benchopt.html"},"url":"https://www.recherche-reproductible.fr/FR-RN/publication/2023/01/09/Benchopt.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://www.recherche-reproductible.fr/FR-RN/feed.xml" title="Recherche Reproductible" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Recherche Reproductible</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/community/">Communauté</a><a class="page-link" href="/activities/">Activités</a><a class="page-link" href="/resources/">Ressources</a><a class="page-link" href="/feed.xml"><svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg>
          </a>

            
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Benchopt: Reproducible, efficient and collaborative optimization benchmarks</h1>
    <p class="post-meta"><time class="dt-published" datetime="2023-01-09T07:00:00+00:00" itemprop="datePublished">
        Jan 9, 2023
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Numerical validation is at the core of machine learning research as it allows us to assess the actual impact of new methods, and to confirm the agreement between theory and practice. Yet, the rapid development of the field poses several challenges: researchers are confronted with a profusion of methods to compare, limited transparency and consensus on best practices, as well as tedious re-implementation work. As a result, validation is often very partial, which can lead to wrong conclusions that slow down the progress of research. We propose Benchopt, a collaborative framework to automatize, publish and reproduce optimization benchmarks in machine learning across programming languages and hardware architectures. Benchopt simplifies benchmarking for the community by providing an off-the-shelf tool for running, sharing and extending experiments. To demonstrate its broad usability, we showcase benchmarks on three standard ML tasks:
L2-regularized logistic regression, Lasso and ResNet18 training for image classification. These benchmarks highlight key practical findings that give a more nuanced view of state-of-the-art for these problems, showing that for practical evaluation, the devil is in the details.</p>

<p><a href="https://openreview.net/forum?id=1uSzacpyWLH"><span id="moreau2022benchopt"><span style="font-variant: small-caps">Moreau, T., Massias, M., Gramfort, A., et al.</span> 2022. Benchopt: Reproducible, efficient and collaborative optimization benchmarks. <i>NeuRIPS</i>.</span></a></p>

  </div><a class="u-url" href="/publication/2023/01/09/Benchopt.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col">
        <b style="color:black;">Recherche Reproductible</b><br/>
        CC-BY 4.0 international license.
        <a href="https://github.com/FR-RN/FR-RN">Sources.</a>
      </div>
      <div class="footer-col">
        <p>Un réseau national de chercheurs pour la promotion de la recherche reproductible.</p>
      </div>
    </div>
  </div>
</footer>
</body>

</html>
